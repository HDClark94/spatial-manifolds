{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pynapple as nap\n",
    "from spatial_manifolds.toroidal import *\n",
    "from spatial_manifolds.behaviour_plots import *\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "from scipy.spatial import distance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap\n",
    "from cebra import CEBRA\n",
    "import cebra.integrations.plotly\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from spatial_manifolds.mlencoding import *\n",
    "from spatial_manifolds.circular_decoder import circular_decoder, cross_validate_decoder, cross_validate_decoder_time, circular_nanmean\n",
    "from spatial_manifolds.data.curation import curate_clusters\n",
    "from scipy.stats import zscore\n",
    "from spatial_manifolds.util import gaussian_filter_nan\n",
    "from spatial_manifolds.predictive_grid import compute_travel_projected, wrap_list\n",
    "from spatial_manifolds.behaviour_plots import *\n",
    "from spatial_manifolds.behaviour_plots import trial_cat_priority\n",
    "from spatial_manifolds.detect_grids import *\n",
    "from spatial_manifolds.brainrender_helper import *\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import hdbscan\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from IPython.display import HTML\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_path = '/Users/harryclark/Documents/figs/FIGURE1/'\n",
    "mouse = 29\n",
    "day = 23\n",
    "\n",
    "# good examples include \n",
    "#mice = [25, 25, 26, 27, 29, 28]\n",
    "#days = [25, 24, 18, 26, 23, 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs, ngs, ns, sc, ngs_ns, all = cell_classification_of1(mouse, day, percentile_threshold=99) # subset\n",
    "rc, rsc, vr_ns = cell_classification_vr(mouse, day)\n",
    "\n",
    "g_m_ids, g_m_cluster_ids = HDBSCAN_grid_modules(gcs, all, mouse, day, min_cluster_size=3, cluster_selection_epsilon=3, \n",
    "                                                figpath=fig_path, curate_with_vr=True, curate_with_brain_region=True) # create grid modules using HDBSCAN    \n",
    "\n",
    "plot_grid_modules_rate_maps(gcs, g_m_ids, g_m_cluster_ids, mouse, day, figpath=fig_path)\n",
    "\n",
    "# we now have cluster ids classified into modules, non grid spatial cells and non spatial cells \n",
    "# as defined by activity in the open field\n",
    "g_m_cluster_ids = sorted(g_m_cluster_ids, key=len, reverse=True) \n",
    "cluster_ids_by_group = []\n",
    "cluster_ids_by_group.extend(g_m_cluster_ids) # grid cells by module [0,1,2...]\n",
    "cluster_ids_by_group.append(ngs.cluster_id.values.tolist()) # non grid spatial [-4]\n",
    "cluster_ids_by_group.append(ns.cluster_id.values.tolist()) # non spatial cells [-3]\n",
    "cluster_ids_by_group.append(gcs.cluster_id.values.tolist()) # all grid cells [-2]\n",
    "cluster_ids_by_group.append(sc.cluster_id.values.tolist()) # speed cells [-1]\n",
    "\n",
    "for m, cluster_ids in enumerate(cluster_ids_by_group):\n",
    "    plot_vr_rate_maps(mouse, day, cluster_ids, label=f'{m}', figpath=fig_path)\n",
    "\n",
    "#plot_vr_rate_maps(mouse, day, rc.cluster_id.values, label=f'ramp_cells', figpath=fig_path)\n",
    "#plot_vr_rate_maps(mouse, day, rsc.cluster_id.values, label=f'speed_ramp_cells', figpath=fig_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ids=cluster_ids_by_group[0]\n",
    "tcs,tcs_time,_,_,_,_ = compute_vr_tcs(mouse, day)\n",
    "ncols = 10\n",
    "nrows = int(np.ceil(len(cluster_ids)/ncols))\n",
    "fig, ax = plt.subplots(ncols=ncols, nrows=nrows, figsize=(10, 1.4*nrows), squeeze=False)\n",
    "counter = 0\n",
    "for j in range(nrows):\n",
    "    for i in range(ncols):\n",
    "        if counter<len(cluster_ids):\n",
    "            index = cluster_ids[counter]\n",
    "            plot_firing_rate_map(ax[j, i], \n",
    "                                zscore(tcs[index]),\n",
    "                                bs=bs,\n",
    "                                tl=tl,\n",
    "                                p=95)\n",
    "        else:\n",
    "            ax[j, i].axis('off')\n",
    "        counter+=1\n",
    "        ax[j, i].set_xticks([])\n",
    "        ax[j, i].set_yticks([])\n",
    "        ax[j, i].xaxis.set_tick_params(labelbottom=False)\n",
    "        ax[j, i].yaxis.set_tick_params(labelleft=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import welch\n",
    "\n",
    "for id in ngs.cluster_id:\n",
    "    cluster = all[all.cluster_id == id]\n",
    "    print(f'{cluster.cluster_id.iloc[0]}, x={np.round(cluster.SC_x.iloc[0])}, y={np.round(cluster.SC_y.iloc[0])}, probe_x={np.round(cluster.probe_x.iloc[0])}, probe_y={np.round(cluster.probe_y.iloc[0])}')\n",
    "    fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(1.5,1.5), squeeze=False)\n",
    "    plot_firing_rate_map(ax[0,0], zscore(tcs[id]), bs=bs, tl=tl, p=95)\n",
    "    plt.show()\n",
    "'''\n",
    "    frequencies, psd = welch(tcs_time[id], fs=int(1000/time_bs))\n",
    "    fig, ax = plt.subplots(figsize=(1.5, 1.5))\n",
    "    ax.plot(frequencies[:50], psd[:50]*frequencies[:50])\n",
    "    ax.set_xlabel('Frequency (Hz)')\n",
    "    ax.set_ylabel('PSD')\n",
    "    plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import welch\n",
    "\n",
    "for id in gcs.cluster_id:\n",
    "    cluster = all[all.cluster_id == id]\n",
    "    print(f'{cluster.cluster_id.iloc[0]}, x={np.round(cluster.SC_x.iloc[0])}, y={np.round(cluster.SC_y.iloc[0])}, probe_x={np.round(cluster.probe_x.iloc[0])}, probe_y={np.round(cluster.probe_y.iloc[0])}')\n",
    "    fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(1.5,1.5), squeeze=False)\n",
    "    plot_firing_rate_map(ax[0,0], zscore(tcs[id]), bs=bs, tl=tl, p=95)\n",
    "    plt.show()\n",
    "'''\n",
    "    frequencies, psd = welch(tcs_time[id], fs=int(1000/time_bs))\n",
    "    fig, ax = plt.subplots(figsize=(1.5, 1.5))\n",
    "    ax.plot(frequencies[:50], psd[:50]*frequencies[:50])\n",
    "    ax.set_xlabel('Frequency (Hz)')\n",
    "    ax.set_ylabel('PSD')\n",
    "    plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(2, 8), squeeze=False)\n",
    "plot_NP2_probe(ax[0,0], sorting_analyzer_path='/Users/harryclark/Downloads/kilosort4_sa')\n",
    "ax[0,0].scatter(ngs['probe_x'], ngs['probe_y'], color='green', alpha=0.3)\n",
    "ax[0,0].scatter(gcs['probe_x'], gcs['probe_y'], color='blue', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pick 2 interesting grid cells to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_grid_cell_id1 = 194\n",
    "reference_grid_cell_id2 = 160\n",
    "reference_non_grid_cell_id3 = 411"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcs, tcs_time, _, last_ephys_bin, beh, clusters = compute_vr_tcs(mouse,day, apply_zscore=False, apply_guassian_filter=False)\n",
    "\n",
    "last_ephys_time_bin = clusters[clusters.index[0]].count(bin_size=time_bs, time_units = 'ms').index[-1]\n",
    "\n",
    "# time binned variables for later\n",
    "ep = nap.IntervalSet(start=0, end=last_ephys_time_bin, time_units = 's')\n",
    "speed_in_time = np.array(beh['S'].bin_average(bin_size=time_bs, time_units = 'ms', ep=ep))\n",
    "dt_in_time = np.array(beh['travel'].bin_average(bin_size=time_bs, time_units = 'ms', ep=ep)-((beh['trial_number'][0]-1)*tl))\n",
    "pos_in_time = dt_in_time%tl\n",
    "trial_number_in_time = (dt_in_time//tl)+beh['trial_number'][0]\n",
    "\n",
    "if np.any(np.isnan(pos_in_time)):\n",
    "    series = pd.Series(dt_in_time)\n",
    "    filled_series = series.ffill().bfill()\n",
    "    dt_in_time = np.array(filled_series)\n",
    "    pos_in_time = dt_in_time%tl\n",
    "    trial_number_in_time = (dt_in_time//tl)+beh['trial_number'][0]\n",
    "\n",
    "if np.any(np.isnan(speed_in_time)):\n",
    "    series = pd.Series(speed_in_time)\n",
    "    filled_series = series.ffill().bfill()\n",
    "    speed_in_time = np.array(speed_in_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "were going to use a history of 2000ms\n",
    "\n",
    "were going to test to see if theres a relationship between the pseudo r2 value when you train on a grid cell to predict the firing of another grid cell and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference_grid_cell_id1\n",
    "ref = np.array(tcs_time[reference_grid_cell_id1])\n",
    "\n",
    "vals1=[]\n",
    "vals2=[]\n",
    "for gc_id in gcs.cluster_id:\n",
    "    alt = np.array(tcs_time[gc_id])\n",
    "    if gc_id != reference_grid_cell_id1:\n",
    "        xgb_history = MLencoding(tunemodel = 'xgboost', cov_history = True, spike_history=False,\n",
    "                         window = time_bs, n_filters = 5, max_time = 2000)\n",
    "        Y_hat, pR2_cv = xgb_history.fit_cv(alt, ref, verbose = 2, continuous_folds = True)\n",
    "        vals1.append(np.nanmean(pR2_cv))\n",
    "        Y_hat, pR2_cv = xgb_history.fit_cv(ref, alt, verbose = 2, continuous_folds = True)\n",
    "        vals2.append(np.nanmean(pR2_cv))\n",
    "plt.scatter(vals1,vals2, color='blue', alpha=0.3)\n",
    "\n",
    "vals1=[]\n",
    "vals2=[]\n",
    "for ngs_id in ngs.cluster_id:\n",
    "    alt = np.array(tcs_time[ngs_id])   \n",
    "    xgb_history = MLencoding(tunemodel = 'xgboost', cov_history = True, spike_history=False,\n",
    "                         window = time_bs, n_filters = 5, max_time = 2000)\n",
    "    Y_hat, pR2_cv = xgb_history.fit_cv(alt, ref, verbose = 2, continuous_folds = True)\n",
    "    vals1.append(np.nanmean(pR2_cv))\n",
    "    Y_hat, pR2_cv = xgb_history.fit_cv(ref, alt, verbose = 2, continuous_folds = True)\n",
    "    vals2.append(np.nanmean(pR2_cv))\n",
    "plt.scatter(vals1,vals2, color='green', alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(vals1,vals2, color='blue')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference_grid_cell_id1\n",
    "ref = np.array(tcs_time[reference_grid_cell_id1])\n",
    "\n",
    "gc_vals1=[]\n",
    "gc_vals2=[]\n",
    "for i, gc_id in enumerate(gcs.cluster_id):\n",
    "    alt = np.array(tcs_time[gc_id])\n",
    "    if gc_id != reference_grid_cell_id1:\n",
    "        xgb_history = MLencoding(tunemodel = 'xgboost', cov_history = True, spike_history=False,\n",
    "                         window = time_bs, n_filters = 5, max_time = 50)\n",
    "        Y_hat, pR2_cv = xgb_history.fit_cv(alt, ref, verbose = 0, continuous_folds = True)\n",
    "        gc_vals1.append(np.nanmean(pR2_cv))\n",
    "        Y_hat, pR2_cv = xgb_history.fit_cv(ref, alt, verbose = 0, continuous_folds = True)\n",
    "        gc_vals2.append(np.nanmean(pR2_cv))\n",
    "        print(f'done {i+1}/{len(gcs)}')\n",
    "\n",
    "ngs_vals1=[]\n",
    "ngs_vals2=[]\n",
    "for i, ngs_id in enumerate(ngs.cluster_id):\n",
    "    alt = np.array(tcs_time[ngs_id])   \n",
    "    xgb_history = MLencoding(tunemodel = 'xgboost', cov_history = True, spike_history=False,\n",
    "                         window = time_bs, n_filters = 5, max_time = 50)\n",
    "    Y_hat, pR2_cv = xgb_history.fit_cv(alt, ref, verbose = 0, continuous_folds = True)\n",
    "    ngs_vals1.append(np.nanmean(pR2_cv))\n",
    "    Y_hat, pR2_cv = xgb_history.fit_cv(ref, alt, verbose = 0, continuous_folds = True)\n",
    "    ngs_vals2.append(np.nanmean(pR2_cv))\n",
    "    print(f'done {i+1}/{len(ngs)}')\n",
    "\n",
    "\n",
    "shuf_vals1=[]\n",
    "shuf_vals2=[]\n",
    "for i, gc_id in enumerate(gcs.cluster_id):\n",
    "    alt = np.array(tcs_time[gc_id])\n",
    "    np.random.shuffle(alt)\n",
    "    if gc_id != reference_grid_cell_id1:\n",
    "        xgb_history = MLencoding(tunemodel = 'xgboost', cov_history = True, spike_history=False,\n",
    "                         window = time_bs, n_filters = 5, max_time = 50)\n",
    "        Y_hat, pR2_cv = xgb_history.fit_cv(alt, ref, verbose = 0, continuous_folds = True)\n",
    "        shuf_vals1.append(np.nanmean(pR2_cv))\n",
    "        Y_hat, pR2_cv = xgb_history.fit_cv(ref, alt, verbose = 0, continuous_folds = True)\n",
    "        shuf_vals2.append(np.nanmean(pR2_cv))\n",
    "        print(f'done {i+1}/{len(gcs)}')\n",
    "\n",
    "plt.scatter(shuf_vals1, shuf_vals2, color='grey', alpha=0.3)\n",
    "plt.axvline(np.nanpercentile(shuf_vals1,95), color='grey', alpha=0.3)\n",
    "plt.axvline(np.nanpercentile(shuf_vals1,99), color='grey', alpha=0.7)\n",
    "plt.axhline(np.nanpercentile(shuf_vals2,95), color='grey', alpha=0.3)\n",
    "plt.axhline(np.nanpercentile(shuf_vals2,99), color='grey', alpha=0.7)\n",
    "plt.scatter(ngs_vals1, ngs_vals2, color='green', alpha=0.3)\n",
    "plt.scatter(gc_vals1,  gc_vals2, color='blue', alpha=0.3)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(shuf_vals1, shuf_vals2, color='grey', alpha=0.3)\n",
    "plt.axvline(np.nanpercentile(shuf_vals1,95), color='grey', alpha=0.3)\n",
    "plt.axvline(np.nanpercentile(shuf_vals1,99), color='grey', alpha=0.7)\n",
    "plt.axhline(np.nanpercentile(shuf_vals2,95), color='grey', alpha=0.3)\n",
    "plt.axhline(np.nanpercentile(shuf_vals2,99), color='grey', alpha=0.7)\n",
    "plt.scatter(ngs_vals1, ngs_vals2, color='green', alpha=0.3)\n",
    "plt.scatter(gc_vals1,  gc_vals2, color='blue', alpha=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference_grid_cell_id1\n",
    "ref = np.array(tcs_time[reference_grid_cell_id1])\n",
    "\n",
    "lh_gc_vals1=[]\n",
    "lh_gc_vals2=[]\n",
    "lh_gc_vals3=[]\n",
    "for i, gc_id in enumerate(gcs.cluster_id):\n",
    "    alt = np.array(tcs_time[gc_id])\n",
    "    if gc_id != reference_grid_cell_id1:\n",
    "        xgb_history = MLencoding(tunemodel = 'xgboost', cov_history = True, spike_history=False,\n",
    "                         window = time_bs, n_filters = 5, max_time = 2000)\n",
    "        Y_hat, pR2_cv = xgb_history.fit_cv(alt, ref, verbose = 0, continuous_folds = True)\n",
    "        lh_gc_vals1.append(np.nanmean(pR2_cv))\n",
    "        Y_hat, pR2_cv = xgb_history.fit_cv(ref, alt, verbose = 0, continuous_folds = True)\n",
    "        lh_gc_vals2.append(np.nanmean(pR2_cv))\n",
    "        dist = distance.euclidean([all[all['cluster_id'] == reference_grid_cell_id1].probe_x.iloc[0],  \n",
    "                                   all[all['cluster_id'] == reference_grid_cell_id1].probe_y.iloc[0]],\n",
    "                                   [all[all['cluster_id'] == gc_id].probe_x.iloc[0],  \n",
    "                                   all[all['cluster_id'] == gc_id].probe_y.iloc[0]])\n",
    "        lh_gc_vals3.append(dist)\n",
    "        print(f'done {i+1}/{len(gcs)}')\n",
    "\n",
    "lh_ngs_vals1=[]\n",
    "lh_ngs_vals2=[]\n",
    "lh_ngs_vals3=[]\n",
    "for i, ngs_id in enumerate(ngs.cluster_id):\n",
    "    alt = np.array(tcs_time[ngs_id])   \n",
    "    xgb_history = MLencoding(tunemodel = 'xgboost', cov_history = True, spike_history=False,\n",
    "                         window = time_bs, n_filters = 5, max_time = 2000)\n",
    "    Y_hat, pR2_cv = xgb_history.fit_cv(alt, ref, verbose = 0, continuous_folds = True)\n",
    "    lh_ngs_vals1.append(np.nanmean(pR2_cv))\n",
    "    Y_hat, pR2_cv = xgb_history.fit_cv(ref, alt, verbose = 0, continuous_folds = True)\n",
    "    lh_ngs_vals2.append(np.nanmean(pR2_cv))\n",
    "    dist = distance.euclidean([all[all['cluster_id'] == reference_grid_cell_id1].probe_x.iloc[0],  \n",
    "                                   all[all['cluster_id'] == reference_grid_cell_id1].probe_y.iloc[0]],\n",
    "                                   [all[all['cluster_id'] == ngs_id].probe_x.iloc[0],  \n",
    "                                   all[all['cluster_id'] == ngs_id].probe_y.iloc[0]])\n",
    "    lh_ngs_vals3.append(dist)\n",
    "    print(f'done {i+1}/{len(ngs)}')\n",
    "\n",
    "\n",
    "lh_shuf_vals1=[]\n",
    "lh_shuf_vals2=[]\n",
    "for i, gc_id in enumerate(gcs.cluster_id):\n",
    "    alt = np.array(tcs_time[gc_id])\n",
    "    np.random.shuffle(alt)\n",
    "    if gc_id != reference_grid_cell_id1:\n",
    "        xgb_history = MLencoding(tunemodel = 'xgboost', cov_history = True, spike_history=False,\n",
    "                         window = time_bs, n_filters = 5, max_time = 50)\n",
    "        Y_hat, pR2_cv = xgb_history.fit_cv(alt, ref, verbose = 0, continuous_folds = True)\n",
    "        lh_shuf_vals1.append(np.nanmean(pR2_cv))\n",
    "        Y_hat, pR2_cv = xgb_history.fit_cv(ref, alt, verbose = 0, continuous_folds = True)\n",
    "        lh_shuf_vals2.append(np.nanmean(pR2_cv))\n",
    "        print(f'done {i+1}/{len(gcs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(lh_shuf_vals1, lh_shuf_vals2, color='grey', alpha=0.3)\n",
    "plt.axvline(np.nanpercentile(lh_shuf_vals1,95), color='grey', alpha=0.3)\n",
    "plt.axvline(np.nanpercentile(lh_shuf_vals1,99), color='grey', alpha=0.7)\n",
    "plt.axhline(np.nanpercentile(lh_shuf_vals2,95), color='grey', alpha=0.3)\n",
    "plt.axhline(np.nanpercentile(lh_shuf_vals2,99), color='grey', alpha=0.7)\n",
    "plt.scatter(lh_ngs_vals1, lh_ngs_vals2, color='green', alpha=0.3)\n",
    "plt.scatter(lh_gc_vals1,  lh_gc_vals2, color='blue', alpha=0.3)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('pR2 - predict ref GC with cell')\n",
    "plt.ylabel('pR2 - predict cell with ref GC')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(lh_ngs_vals1, lh_ngs_vals3, color='green', alpha=0.3)\n",
    "plt.scatter(lh_gc_vals1,  lh_gc_vals3, color='blue', alpha=0.3)\n",
    "plt.xscale('log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_id1 = cluster_ids_by_group[0][9]\n",
    "gc_id2 = cluster_ids_by_group[0][10]\n",
    "distal_id = 316\n",
    "\n",
    "y1 = np.array(tcs_time[gc_id1])\n",
    "y2 = np.array(tcs_time[gc_id2])\n",
    "y3 = np.array(tcs_time[distal_id])\n",
    "\n",
    "X  = np.stack([pos_in_time, speed_in_time]).T\n",
    "X1 = np.stack([pos_in_time, speed_in_time, y1]).T\n",
    "X2 = np.stack([pos_in_time, speed_in_time, y2]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_history = MLencoding(tunemodel = 'xgboost',\n",
    "                         cov_history = True, spike_history=True, # We can choose!\n",
    "                         window = time_bs, #this dataset has 100ms time bins\n",
    "                         n_filters = 5,\n",
    "                         max_time = 1000)\n",
    "\n",
    "Y_hat, pR2_cv = xgb_history.fit_cv(X, y1, verbose = 2, continuous_folds = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot to see if the true time binned spikes aligns with the prediction of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y1, Y_hat, alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the rate maps for true space binned, true time binned and predicted time binned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(2, 2), squeeze=False)\n",
    "plot_firing_rate_map(ax[0,0], zscore(tcs[gc_id1]), bs=bs, tl=tl, p=95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(2, 2), squeeze=False)\n",
    "plot_firing_rate_map(ax[0,0], zscore(tcs[gc_id2]), bs=bs, tl=tl, p=95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(2, 2), squeeze=False)\n",
    "plot_firing_rate_map(ax[0,0], zscore(tcs[distal_id]), bs=bs, tl=tl, p=95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(3, 3), squeeze=False)\n",
    "plot_firing_rate_map(ax[0,0], zscore(tcs[16]), bs=bs, tl=tl, p=95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(4, 2), squeeze=False)\n",
    "tc, _ = np.histogram(dt_in_time, weights=y1, bins=last_ephys_bin, range=(0, bs*last_ephys_bin))\n",
    "plot_firing_rate_map(ax[0,0], zscore(tc), bs=bs, tl=tl, p=95)\n",
    "tc, _ = np.histogram(dt_in_time, weights=Y_hat, bins=last_ephys_bin, range=(0, bs*last_ephys_bin))\n",
    "plot_firing_rate_map(ax[0,1], zscore(tc), bs=bs, tl=tl, p=95)\n",
    "ax[0,0].set_title(f'actual')\n",
    "ax[0,1].set_title(f'predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to evaluate the effect of the history parameter. Here we will increase the number of time bins available to the sample\n",
    "and plot this as a function of model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins_history = np.append(np.append(np.array([5,6,7,8,9,10]), np.arange(10,100,20)), np.arange(100,501, 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pR2s = []\n",
    "for nbins in nbins_history:\n",
    "    xgb_history = MLencoding(tunemodel = 'xgboost', cov_history = True, spike_history=True, \n",
    "                             window = time_bs, n_filters = 5, max_time = int(time_bs*nbins))\n",
    "    Y_hat, pR2_cv = xgb_history.fit_cv(X, y1, verbose = 0, continuous_folds = True)\n",
    "    pR2s.append(np.nanmean(pR2_cv))\n",
    "plt.plot(nbins_history, pR2s)\n",
    "plt.xlabel('history length (bins)')\n",
    "plt.ylabel('pseudo R2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set the coviarate history to false and the spike history to true. This is only testing prediction using the cells own history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pR2s = []\n",
    "for nbins in nbins_history:\n",
    "    xgb_history = MLencoding(tunemodel = 'xgboost', cov_history = False, spike_history=True, \n",
    "                             window = time_bs, n_filters = 5, max_time = int(time_bs*nbins))\n",
    "    Y_hat, pR2_cv = xgb_history.fit_cv(X, y1, verbose = 0, continuous_folds = True)\n",
    "    pR2s.append(np.nanmean(pR2_cv))\n",
    "plt.plot(nbins_history, pR2s)\n",
    "plt.xlabel('history length (bins)')\n",
    "plt.ylabel('pseudo R2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set the spike history to false and the cov history to true, this is now only using spike data from the other cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pR2s = []\n",
    "for nbins in nbins_history:\n",
    "    xgb_history = MLencoding(tunemodel = 'xgboost', cov_history = True, spike_history=False, \n",
    "                             window = time_bs, n_filters = 5, max_time = int(time_bs*nbins))\n",
    "    Y_hat, pR2_cv = xgb_history.fit_cv(y2, y1, verbose = 0, continuous_folds = True)\n",
    "    pR2s.append(np.nanmean(pR2_cv))\n",
    "plt.plot(nbins_history, pR2s)\n",
    "plt.xlabel('history length (bins)')\n",
    "plt.ylabel('pseudo R2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set the spike history to false and the cov history to true, We will now look at the effect of using covariate history from a cell with a very distal connection (such as in the cerebellum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pR2s = []\n",
    "for nbins in nbins_history:\n",
    "    xgb_history = MLencoding(tunemodel = 'xgboost', cov_history = True, spike_history=False, \n",
    "                             window = time_bs, n_filters = 5, max_time = int(time_bs*nbins))\n",
    "    Y_hat, pR2_cv = xgb_history.fit_cv(y3, y1, verbose = 0, continuous_folds = True)\n",
    "    pR2s.append(np.nanmean(pR2_cv))\n",
    "plt.plot(nbins_history, pR2s)\n",
    "plt.xlabel('history length (bins)')\n",
    "plt.ylabel('pseudo R2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like for a number of different runs, providing a spike history of 40 time bins is optimal for model performance.\n",
    "\n",
    "Now we're going to test model performance by dropping out n numbers of grid cells from a covariate history and see how the number of comodular grid cells improves the prediction of a held out grid cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_grid_cell = cluster_ids_by_group[0][9]\n",
    "grid_module_population_cluster_id_minus_test_grid_cells = cluster_ids_by_group[0] \n",
    "grid_module_population_cluster_id_minus_test_grid_cells.remove(test_grid_cell)\n",
    "x_grid_module = {cluster_id: tcs_time[cluster_id] for cluster_id in grid_module_population_cluster_id_minus_test_grid_cells if cluster_id in tcs_time}\n",
    "\n",
    "all_x = np.vstack(list(x_grid_module.values())).T\n",
    "y = np.array(tcs_time[test_grid_cell])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = np.arange(1, all_x.shape[1]+1, 2)\n",
    "iterations = 3\n",
    "ntime_bins = 5\n",
    "nfilters = 5\n",
    "pR2s = np.zeros((len(n_neurons), iterations))\n",
    "pR2s_shuff = np.zeros((len(n_neurons), iterations))\n",
    "\n",
    "for n in n_neurons:\n",
    "    print(f'I am going to use only {n} grid cells')\n",
    "    reps = []\n",
    "    for i in range(iterations):\n",
    "        # real data\n",
    "        xgb_history = MLencoding(tunemodel = 'xgboost', cov_history = True, spike_history=False, \n",
    "                             window = time_bs, n_filters = nfilters, max_time = 2000)\n",
    "        np.random.seed(i)\n",
    "        idx = np.random.choice(n_neurons-1, n, replace=False)\n",
    "        x = all_x[:, idx]\n",
    "        Y_hat, pR2_cv = xgb_history.fit_cv(x, y, verbose = 0, continuous_folds = True)\n",
    "        pR2s[n-1, i] = np.nanmean(pR2_cv)\n",
    "        print(f'pR2_cv = {np.nanmean(pR2_cv)}')\n",
    "\n",
    "        # shuffle data\n",
    "        xgb_history = MLencoding(tunemodel = 'xgboost', cov_history = True, spike_history=False, \n",
    "                             window = time_bs, n_filters = nfilters, max_time = 2000)\n",
    "        nx = x.T.copy()\n",
    "        for r, xROW in enumerate(x.T):\n",
    "            np.random.shuffle(xROW)\n",
    "            nx[r]=xROW\n",
    "        x = nx.T\n",
    "\n",
    "        Y_hat, pR2_cv = xgb_history.fit_cv(x, y, verbose = 0, continuous_folds = True)\n",
    "        pR2s_shuff[n-1, i] = np.nanmean(pR2_cv)\n",
    "        print(f'pR2_cv_shuff = {np.nanmean(pR2_cv)}')\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(2,2))\n",
    "\n",
    "ax.plot(n_neurons, np.nanmean(pR2s, axis=1), color='red', label='real')\n",
    "ax.fill_between(n_neurons, \n",
    "         np.nanmean(pR2s, axis=1)+stats.sem(pR2s, axis=1, nan_policy='omit'),\n",
    "         np.nanmean(pR2s, axis=1)-stats.sem(pR2s, axis=1, nan_policy='omit'),color='red', alpha=0.3)\n",
    "ax.plot(n_neurons, np.nanmean(pR2s_shuff, axis=1), color='grey', label='shuffle')\n",
    "ax.fill_between(n_neurons, \n",
    "         np.nanmean(pR2s_shuff, axis=1)+stats.sem(pR2s_shuff, axis=1, nan_policy='omit'),\n",
    "         np.nanmean(pR2s_shuff, axis=1)-stats.sem(pR2s_shuff, axis=1, nan_policy='omit'),color='grey', alpha=0.3)\n",
    "ax.set_ylabel(f'pseudo R2')\n",
    "ax.set_xlabel(f'n neurons')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can clear see that any number of grid cells can beat the shuffle, but there is slowing of the plateau at around 5 grid cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what happens if we try to predict the activity of a cell with the history of another cell, and do this for all combinations within a session?\n",
    "We can sorted the clusters by ML position in this session because we are recording across all the shanks. \n",
    "\n",
    "First we will only use 50ms of history (5 time bins of 10 ms each). \n",
    "\n",
    "Then we will use 2 seconds of history (200 time bins of 10 ms each).\n",
    "\n",
    "We'll take an example grid cell from above and test the predictabliltity against all other cells first before running every combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(tcs_time[gc_id2])[:5000]\n",
    "\n",
    "all = all.sort_values(by=['SC_x', 'SC_y'], ascending=[False, False])\n",
    "\n",
    "nfilters = 5\n",
    "pR2s_short_history = np.zeros(len(all))\n",
    "pR2s_long_history = np.zeros(len(all))\n",
    "\n",
    "done = 0\n",
    "n_to_finish = len(all)\n",
    "\n",
    "for i, id_i in enumerate(all.cluster_id):\n",
    "    x = np.array(tcs_time[id_i])[:5000]\n",
    "\n",
    "    xgb_history = MLencoding(tunemodel = 'xgboost', cov_history = True, spike_history=False, \n",
    "                                window = time_bs, n_filters = nfilters, max_time = 50)\n",
    "    Y_hat, pR2_cv = xgb_history.fit_cv(x, y, verbose = 0, continuous_folds = True)\n",
    "    pR2s_short_history[i] = np.nanmean(pR2_cv)\n",
    "    \n",
    "    xgb_history = MLencoding(tunemodel = 'xgboost', cov_history = True, spike_history=False, \n",
    "                                window = time_bs, n_filters = nfilters, max_time = 2000)\n",
    "    Y_hat, pR2_cv = xgb_history.fit_cv(x, y, verbose = 0, continuous_folds = True)\n",
    "    pR2s_long_history[i] = np.nanmean(pR2_cv)\n",
    "    \n",
    "    done+= 1\n",
    "    print(f'completed {done}/{n_to_finish}')\n",
    "\n",
    "plt.plot(pR2s_short_history, label='50ms history')\n",
    "plt.plot(pR2s_long_history, label='2000ms history')\n",
    "plt.legend()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(all.SC_x, all.SC_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = all.sort_values(by=['SC_x', 'SC_y'], ascending=[False, False])\n",
    "\n",
    "cap = True\n",
    "nfilters = 5\n",
    "pR2s = np.zeros((len(all), 4))\n",
    "\n",
    "done = 0\n",
    "n_to_finish = len(all)*4\n",
    "\n",
    "for i, id_i in enumerate(all.cluster_id):\n",
    "    for j, (left, right) in enumerate(zip([-3800, -3500, -3300, -3100], [-3600, -3400,-3100, -2900])):\n",
    "        xgb_history = MLencoding(tunemodel = 'xgboost', cov_history = True, spike_history=False, \n",
    "                                 window = time_bs, n_filters = nfilters, max_time = 50)\n",
    "        y = np.array(tcs_time[id_i])\n",
    "\n",
    "        valid_cluster_ids = all[(all['SC_x'] > left) & \n",
    "                                (all['SC_x'] < right) & \n",
    "                                (all['cluster_id'] != id_i)]\n",
    "        tcs_to_use = {cluster_id: tcs_time[cluster_id] for cluster_id in valid_cluster_ids.cluster_id.values if cluster_id in tcs_time}\n",
    "        x = np.vstack(list(tcs_to_use.values())).T\n",
    "        \n",
    "        if cap:\n",
    "            x = x[:5000,:]\n",
    "            y = y[:5000]\n",
    "\n",
    "        Y_hat, pR2_cv = xgb_history.fit_cv(x, y, verbose = 0, continuous_folds = True)\n",
    "        pR2s[i, j] = np.nanmean(pR2_cv)\n",
    "\n",
    "        done+= 1\n",
    "        print(f'completed {done}/{n_to_finish}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = all.sort_values(by=['SC_x', 'SC_y'], ascending=[False, False])\n",
    "\n",
    "cap = True\n",
    "nfilters = 5\n",
    "pR2slh = np.zeros((len(all), 4))\n",
    "\n",
    "done = 0\n",
    "n_to_finish = len(all)*4\n",
    "\n",
    "for i, id_i in enumerate(all.cluster_id):\n",
    "    for j, (left, right) in enumerate(zip([-3800, -3500, -3300, -3100], [-3600, -3400,-3100, -2900])):\n",
    "        xgb_history = MLencoding(tunemodel = 'xgboost', cov_history = True, spike_history=False, \n",
    "                                 window = time_bs, n_filters = nfilters, max_time = 2000)\n",
    "        y = np.array(tcs_time[id_i])\n",
    "\n",
    "        valid_cluster_ids = all[(all['SC_x'] > left) & \n",
    "                                (all['SC_x'] < right) & \n",
    "                                (all['cluster_id'] != id_i)]\n",
    "        tcs_to_use = {cluster_id: tcs_time[cluster_id] for cluster_id in valid_cluster_ids.cluster_id.values if cluster_id in tcs_time}\n",
    "        x = np.vstack(list(tcs_to_use.values())).T\n",
    "        \n",
    "        if cap:\n",
    "            x = x[:5000,:]\n",
    "            y = y[:5000]\n",
    "\n",
    "        Y_hat, pR2_cv = xgb_history.fit_cv(x, y, verbose = 0, continuous_folds = True)\n",
    "        pR2slh[i, j] = np.nanmean(pR2_cv)\n",
    "\n",
    "        done+= 1\n",
    "        print(f'completed {done}/{n_to_finish}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(3,2),sharey=True)\n",
    "\n",
    "x = np.arange(1, len(pR2slh)+1)\n",
    "y = np.arange(0, len(pR2slh[0]))\n",
    "X, Y = np.meshgrid(x, y)\n",
    "heatmap = ax[0].pcolormesh(Y, X, pR2s.T, shading='auto', cmap='viridis', vmin=0, vmax=0.4)\n",
    "heatmap.set_rasterized(True)\n",
    "ax[0].set_ylabel(f'Tested on neuron')\n",
    "ax[0].set_xlabel(f'Trained on shank')\n",
    "ax[0].set_xticks([0,1,2,3])\n",
    "ax[0].set_xticklabels([1,2,3,4])\n",
    "ax[0].invert_yaxis()\n",
    "ax[0].set_title('50ms cov history')\n",
    "\n",
    "heatmap = ax[1].pcolormesh(Y, X, pR2slh.T, shading='auto', cmap='viridis', vmin=0,vmax=0.4)\n",
    "heatmap.set_rasterized(True)\n",
    "ax[1].set_xlabel(f'Trained on shank')\n",
    "ax[1].set_xticks([0,1,2,3])\n",
    "ax[1].set_xticklabels([1,2,3,4])\n",
    "ax[1].set_title('2s cov history')\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = all.sort_values(by=['SC_x', 'SC_y'], ascending=[False, False])\n",
    "\n",
    "cap = True\n",
    "nfilters = 5\n",
    "pR2s_spike_and_covariate_history = np.zeros((len(all), 4))\n",
    "\n",
    "done = 0\n",
    "n_to_finish = len(all)*4\n",
    "\n",
    "for i, id_i in enumerate(all.cluster_id):\n",
    "    for j, (left, right) in enumerate(zip([-3800, -3500, -3300, -3100], [-3600, -3400,-3100, -2900])):\n",
    "        xgb_history = MLencoding(tunemodel = 'xgboost', cov_history = True, spike_history=True, \n",
    "                                 window = time_bs, n_filters = nfilters, max_time = 50)\n",
    "        y = np.array(tcs_time[id_i])\n",
    "\n",
    "        valid_cluster_ids = all[(all['SC_x'] > left) & \n",
    "                                (all['SC_x'] < right) & \n",
    "                                (all['cluster_id'] != id_i)]\n",
    "        tcs_to_use = {cluster_id: tcs_time[cluster_id] for cluster_id in valid_cluster_ids.cluster_id.values if cluster_id in tcs_time}\n",
    "        x = np.vstack(list(tcs_to_use.values())).T\n",
    "        \n",
    "        if cap:\n",
    "            x = x[:5000, :]\n",
    "            y = y[:5000]\n",
    "\n",
    "        Y_hat, pR2_cv = xgb_history.fit_cv(x, y, verbose = 0, continuous_folds = True)\n",
    "        pR2s_spike_and_covariate_history[i, j] = np.nanmean(pR2_cv)\n",
    "        done+= 1\n",
    "        print(f'completed {done}/{n_to_finish}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = all.sort_values(by=['SC_x', 'SC_y'], ascending=[False, False])\n",
    "\n",
    "cap = True\n",
    "nfilters = 5\n",
    "pR2s_spike_history = np.zeros((len(all), 4))\n",
    "\n",
    "done = 0\n",
    "n_to_finish = len(all)*4\n",
    "\n",
    "for i, id_i in enumerate(all.cluster_id):\n",
    "    for j, (left, right) in enumerate(zip([-3800, -3500, -3300, -3100], [-3600, -3400,-3100, -2900])):\n",
    "        xgb_history = MLencoding(tunemodel = 'xgboost', cov_history = False, spike_history=True, \n",
    "                                 window = time_bs, n_filters = nfilters, max_time = 50)\n",
    "        y = np.array(tcs_time[id_i])\n",
    "\n",
    "        valid_cluster_ids = all[(all['SC_x'] > left) & \n",
    "                                (all['SC_x'] < right) & \n",
    "                                (all['cluster_id'] != id_i)]\n",
    "        tcs_to_use = {cluster_id: tcs_time[cluster_id] for cluster_id in valid_cluster_ids.cluster_id.values if cluster_id in tcs_time}\n",
    "        x = np.vstack(list(tcs_to_use.values())).T\n",
    "        \n",
    "        if cap:\n",
    "            x = x[:5000,:]\n",
    "            y = y[:5000]\n",
    "\n",
    "        Y_hat, pR2_cv = xgb_history.fit_cv(x, y, verbose = 0, continuous_folds = True)\n",
    "        pR2s_spike_history[i, j] = np.nanmean(pR2_cv)\n",
    "        done+= 1\n",
    "        print(f'completed {done}/{n_to_finish}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, nrows=1, figsize=(5,2),sharey=True)\n",
    "\n",
    "x = np.arange(1, len(pR2slh)+1)\n",
    "y = np.arange(0, len(pR2slh[0]))\n",
    "X, Y = np.meshgrid(x, y)\n",
    "heatmap = ax[0].pcolormesh(Y, X, pR2s_spike_history.T, shading='auto', cmap='viridis', vmin=0, vmax=0.3)\n",
    "heatmap.set_rasterized(True)\n",
    "ax[0].set_ylabel(f'Tested on neuron')\n",
    "ax[0].set_xlabel(f'Trained on S')\n",
    "ax[0].set_xticks([0,1,2,3])\n",
    "ax[0].set_xticklabels([1,2,3,4])\n",
    "ax[0].invert_yaxis()\n",
    "ax[0].set_title('sp history')\n",
    "\n",
    "heatmap = ax[1].pcolormesh(Y, X, pR2s_spike_and_covariate_history.T, shading='auto', cmap='viridis', vmin=0, vmax=0.3)\n",
    "heatmap.set_rasterized(True)\n",
    "ax[1].set_xlabel(f'Trained on S')\n",
    "ax[1].set_xticks([0,1,2,3])\n",
    "ax[1].set_xticklabels([1,2,3,4])\n",
    "ax[1].set_title('sp&cov history')\n",
    "\n",
    "heatmap = ax[2].pcolormesh(Y, X, pR2s.T, shading='auto', cmap='viridis', vmin=0, vmax=0.3)\n",
    "heatmap.set_rasterized(True)\n",
    "ax[2].set_xlabel(f'Trained on S')\n",
    "ax[2].set_xticks([0,1,2,3])\n",
    "ax[2].set_xticklabels([1,2,3,4])\n",
    "ax[2].set_title('cov history')\n",
    "\n",
    "\n",
    "heatmap = ax[3].pcolormesh(Y, X, pR2s_spike_and_covariate_history.T-pR2s_spike_history.T, shading='auto', cmap='coolwarm', vmin=-0.1, vmax=0.1)\n",
    "heatmap.set_rasterized(True)\n",
    "ax[3].set_xlabel(f'Trained on S')\n",
    "ax[3].set_xticks([0,1,2,3])\n",
    "ax[3].set_xticklabels([1,2,3,4])\n",
    "ax[3].set_title('spcov minus sp')\n",
    "\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = all.sort_values(by=['SC_x'], ascending=[False])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pR2s, cmap='viridis', vmin=0, vmax=0.2)\n",
    "plt.colorbar()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = all.sort_values(by=['SC_x', 'SC_y'], ascending=[False, False])\n",
    "\n",
    "nfilters = 5\n",
    "pR2s = np.zeros((len(all), len(all)))\n",
    "\n",
    "done = 0\n",
    "n_to_finish = len(all)*len(all)\n",
    "\n",
    "for i, id_i in enumerate(all.cluster_id):\n",
    "    for j, id_j in enumerate(all.cluster_id):\n",
    "        xgb_history = MLencoding(tunemodel = 'xgboost', cov_history = True, spike_history=False, \n",
    "                                 window = time_bs, n_filters = nfilters, max_time = 2000)\n",
    "        x = np.array(tcs_time[id_i][:50000])\n",
    "        y = np.array(tcs_time[id_j][:50000])\n",
    "        Y_hat, pR2_cv = xgb_history.fit_cv(x, y, verbose = 0, continuous_folds = True)\n",
    "        pR2s[i, j] = np.nanmean(pR2_cv)\n",
    "\n",
    "        done+= 1\n",
    "        print(f'completed {done}/{n_to_finish}')\n",
    "\n",
    "plt.imshow(pR2s, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, hp, Trials, tpe, STATUS_OK\n",
    "\n",
    "# Makes sure these are in model.params, otherwise you'll get a key error\n",
    "space4rf = {\n",
    "    'silent': 1,\n",
    "    'learning_rate': hp.choice('learning_rate', [0.01, 0.05, 0.1, 0.2, 0.3]),\n",
    "    'min_child_weight': hp.choice('min_child_weight', [1, 2, 5, 10]),\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 300, 500, 700, 1000]),\n",
    "    'subsample': hp.choice('subsample', [0.5, 0.6, 0.7, 0.8, 1.0]),\n",
    "    'max_depth': hp.choice('max_depth', [3, 5, 7, 9]),\n",
    "    'gamma': hp.choice('gamma', [0, 0.1, 0.3, 0.5, 1.0]),\n",
    "}\n",
    "\n",
    "#object that holds iteration results\n",
    "trials = Trials()\n",
    "\n",
    "#define model\n",
    "xgb_history = MLencoding(tunemodel = 'xgboost',\n",
    "                         cov_history = False, spike_history=True, # We can choose!\n",
    "                         window = time_bs, #this dataset has 50ms time bins\n",
    "                         n_filters = 5,\n",
    "                         max_time = int(time_bs*10))\n",
    "\n",
    "#function to minimize\n",
    "def fnc(params):\n",
    "    print(params)\n",
    "\n",
    "    # make sure parameters are integers that need to be. \n",
    "    params['silent'] = int(params['silent'])\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['min_child_weight'] = int(params['min_child_weight'])\n",
    "\n",
    "    xgb_history.set_params(params)\n",
    "    \n",
    "    # Remember that X and y have been defined above.\n",
    "    Y_hat, PR2s = xgb_history.fit_cv(X,y, n_cv = 5, verbose = 0, continuous_folds = True)\n",
    "\n",
    "    # return negative since hyperopt always minimizes the function\n",
    "    return -np.mean(PR2s)\n",
    "\n",
    "hyperoptBest = fmin(fnc, space4rf, algo=tpe.suggest, max_evals=500, \n",
    "                    trials=trials, return_argmin=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperoptBest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
